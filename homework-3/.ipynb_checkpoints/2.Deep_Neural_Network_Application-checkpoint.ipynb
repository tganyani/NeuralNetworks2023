{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокая нейронная сеть для классификации изображений: применение\n",
    "\n",
    "Когда вы закончите это, вы выполните последнее задание по ручному программированию нейронных сетей!\n",
    "\n",
    "Вы будете использовать функции, которые вы реализовали в предыдущем задании, для построения глубокой сети и применять ее для классификации кошек и не кошек. Надеемся, вы увидите улучшение точности по сравнению с предыдущей реализацией логистической регрессии.\n",
    "\n",
    "**После этого задания вы сможете:**  \n",
    "- Создадите и примените глубокую нейронную сеть для контролируемого обучения.\n",
    "\n",
    "Давайте начнем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Пакеты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сначала импортируем все пакеты, которые вам понадобятся во время этого задания.\n",
    "- [numpy](www.numpy.org) — фундаментальный пакет для научных вычислений на Python.\n",
    "- [matplotlib](http://matplotlib.org) — библиотека для построения графиков на Python.\n",
    "- [h5py](http://www.h5py.org) — это общий пакет для взаимодействия с набором данных, хранящимся в файле H5.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) и [scipy](https://www.scipy.org/) используются здесь для проверки вашей модели с вашим собственным изображением в конце.\n",
    "- dnn_app_utils предоставляет функции, реализованные в задании «Создание глубокой нейронной сети: шаг за шагом» для этого блокнота.\n",
    "- np.random.seed(1) используется для обеспечения согласованности всех вызовов случайных функций. Это поможет нам оценить вашу работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from dnn_app_utils_v3 import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Набор данных\n",
    "\n",
    "Вы будете использовать набор данных «Кошка/не-кошка».\n",
    "\n",
    "**Постановка задачи**: Вам предоставлен набор данных (\"data.h5\"), содержащий:\n",
    "     - обучающий набор изображений m_train, помеченных как кошки (1) или не кошки (0)\n",
    "     - тестовый набор изображений m_test, помеченных как кошки и не кошки\n",
    "     - каждое изображение имеет форму (num_px, num_px, 3), где 3 соответствует 3 каналам (RGB).\n",
    "\n",
    "Давайте познакомимся с набором данных поближе. Загрузите данные, запустив ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующий код покажет вам изображение из набора данных. Не стесняйтесь менять индекс и повторно запускать ячейку несколько раз, чтобы увидеть другие изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как обычно, вы изменяете и стандартизируете изображения перед отправкой их в нейронную сеть. Код указан в ячейке ниже.\n",
    "\n",
    "<img src=\"images/imvectorkiank.png\" style=\"width:450px;height:300px;\">\n",
    "\n",
    "<caption><center> <u>Figure 1</u>: Конвертация картинки в вектор. <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$12 288$ равны $64 \\times 64 \\times 3$, что соответствует размеру одного вектора измененного изображения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Архитектура вашей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда вы знакомы с набором данных, пришло время построить глубокую нейронную сеть, чтобы отличать изображения кошек от изображений, не относящихся к кошкам.\n",
    "\n",
    "Вы построите две разные модели:\n",
    "- Двухслойная нейронная сеть\n",
    "- Глубокая нейронная сеть из L слоёв.\n",
    "\n",
    "Затем вы сравните производительность этих моделей, а также опробуете разные значения $L$.\n",
    "\n",
    "Давайте посмотрим на две архитектуры.\n",
    "\n",
    "### 3.1 - 2-слойная нейронная сеть\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Рис. 2</u>: двухслойная нейронная сеть. <br> Модель можно резюмировать следующим образом: ***ВХОД -> ЛИНЕЙНЫЙ -> RELU -> ЛИНЕЙНЫЙ -> СИГМОИДА -> ВЫХОД***. </center></caption>\n",
    "\n",
    "<u>Подробная архитектура рисунка 2</u>:\n",
    "- Входные данные представляют собой изображение (64, 64, 3), преобразованное в вектор размера $(12288, 1)$.\n",
    "- Соответствующий вектор: $[x_0,x_1,...,x_{12287}]^T$ умножается на весовую матрицу $W^{[1]}$ размера $(n^{[1]} , 12288)$.\n",
    "- Затем вы добавляете смещение и берете от него relu, чтобы получить следующий вектор: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1 }^{[1]}]^T$.\n",
    "- Затем вы повторяете тот же процесс.\n",
    "- Вы умножаете полученный вектор на $W^{[2]}$ и добавляете смещение.\n",
    "- Наконец, вы берете сигмоиду от результата. Если оно больше 0.5, вы классифицируете его как кошку.\n",
    "\n",
    "### 3.2 - Глубокая L-слойная нейронная сеть\n",
    "\n",
    "Трудно представить глубокую L-слойную нейронную сеть с помощью приведенного выше представления. Однако вот упрощенное представление сети:\n",
    "\n",
    "<img src=\"images/LlayerNN_kiank.png\" style=\"width:650px;height:400px;\">\n",
    "<caption><center> <u>Рис. 3</u>: L-слойная нейронная сеть. <br> Модель можно резюмировать следующим образом: ***[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID***</center></caption>\n",
    "\n",
    "<u>Подробная архитектура рисунка 3</u>:\n",
    "- Входные данные представляют собой изображение (64, 64, 3), преобразованное в вектор размера (12288, 1).\n",
    "- Соответствующий вектор: $[x_0,x_1,...,x_{12287}]^T$ умножается на весовую матрицу $W^{[1]}$, а затем добавляется смещение $b^{[ 1]}$. Эта часть является линейной.\n",
    "- Далее берёте relu. Этот процесс может повторяться несколько раз для каждого $(W^{[l]}, b^{[l]})$ в зависимости от архитектуры модели.\n",
    "- Наконец, вы берете сигмоиду от конечного линейного преобразования. Если оно больше 0.5, вы классифицируете его как кошку.\n",
    "\n",
    "### 3.3 - Общая методология\n",
    "\n",
    "Как обычно, для построения модели вы будете следовать методологии глубокого обучения:\n",
    " 1. Инициализация параметров/Определение гиперпараметров.\n",
    " 2. Цикл по num_iterations:\n",
    "     - а. Прямое распространение\n",
    "     - б. Вычислить функцию стоимости\n",
    "     - в. Обратное распространение\n",
    "     - д. Обновить параметры (с использованием параметров и оценок из резервной копии)\n",
    " 4. Использование нейронной сети с обученными параметрами для предсказания.\n",
    "\n",
    "Давайте реализуем эти две модели!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Двухслойная нейронная сеть\n",
    "\n",
    "**Задание**: используйте вспомогательные функции, которые вы реализовали в предыдущем задании, для построения двухслойной нейронной сети следующей структуры: *LINEAR -> RELU -> LINEAR -> SIGMOID*. Функции, которые вам могут понадобиться, и их входные данные:\n",
    "```python\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    ...\n",
    "    return parameters \n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    ...\n",
    "    return A, cache\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    ...\n",
    "    return dA_prev, dW, db\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = None\n",
    "        A2, cache2 = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = None\n",
    "        dA0, dW1, db1 = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустите ячейку ниже, чтобы обучить свои параметры. Посмотрите, работает ли ваша модель. Ошибка должна снизиться. Выполнение 2500 итераций может занять до 5 минут. Проверьте, соответствует ли «Ошибка после итерации 0» ожидаемому результату ниже, если нет, щелкните квадрат (⬛) на верхней панели блокнота, чтобы остановить ячейку и попытаться найти ошибку в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый вывод**:\n",
    "<table> \n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.6930497356599888 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 100**</td>\n",
    "        <td> 0.6464320953428849 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.048554785628770206 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, что вы создали векторизованную реализацию! В противном случае обучение могло бы занять в 10 раз больше времени.\n",
    "\n",
    "Теперь вы можете использовать обученные параметры для классификации изображений из набора данных. Чтобы увидеть свои прогнозы по обучающим и тестовым наборам, запустите ячейку ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый вывод**:\n",
    "<table> \n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 1.0 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый вывод**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td> **Accuracy**</td>\n",
    "        <td> 0.72 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Вы можете заметить, что выполнение модели за меньшее количество итераций (скажем, 1500) обеспечивает более высокую точность на тестовом наборе. Это называется «ранняя остановка», и об этом мы поговорим в следующем курсе. Ранняя остановка – это способ предотвратить переобучение.\n",
    "\n",
    "Поздравляем! Кажется, что ваша двухслойная нейронная сеть имеет лучшую производительность (72%), чем реализация логистической регрессии (70%). Давайте посмотрим, сможете ли вы добиться еще большего с помощью модели из $L$ слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - L-слойная нейронная сеть\n",
    "\n",
    "**Задание**: используйте вспомогательные функции, которые вы реализовали ранее, чтобы построить $L$-слойную нейронную сеть со следующей структурой: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Функции, которые вам могут понадобиться, и их входные данные:\n",
    "```python\n",
    "def initialize_parameters_deep(layers_dims):\n",
    "    ...\n",
    "    return parameters \n",
    "def L_model_forward(X, parameters):\n",
    "    ...\n",
    "    return AL, caches\n",
    "def compute_cost(AL, Y):\n",
    "    ...\n",
    "    return cost\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    ...\n",
    "    return grads\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = None\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = None\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = None\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь вы обучите модель 4-слойной нейронной сети.\n",
    "\n",
    "Запустите ячейку ниже, чтобы обучить свою модель. Ошибка обучения должна уменьшаться на каждой итерации. Выполнение 2500 итераций может занять до 5 минут. Проверьте, соответствует ли «Ошибка обучения после итерации 0» ожидаемому результату ниже, если нет, щелкните квадрат (⬛) на верхней панели блокнота, чтобы остановить ячейку и попытаться найти ошибку в коде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый вывод**:\n",
    "<table> \n",
    "    <tr>\n",
    "        <td> **Cost after iteration 0**</td>\n",
    "        <td> 0.771749 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 100**</td>\n",
    "        <td> 0.672053 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **...**</td>\n",
    "        <td> ... </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> **Cost after iteration 2400**</td>\n",
    "        <td> 0.092878 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **Train Accuracy**\n",
    "    </td>\n",
    "    <td>\n",
    "    0.985645933014\n",
    "    </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ожидаемый вывод**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td> **Test Accuracy**</td>\n",
    "        <td> 0.8 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поздравляю! Кажется, что ваша 4-слойная нейронная сеть имеет лучшую производительность (80%), чем ваша 2-слойная нейронная сеть (72%) на том же тестовом наборе.\n",
    "\n",
    "Это хорошая производительность для данной задачи. Хорошая работа!\n",
    "\n",
    "Однако вы знаете, как получить еще более высокую точность - систематически ища лучшие гиперпараметры: learning_rate, Layers_dims, num_iterations и другие, о которых вы также узнаете в следующих уроках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Анализ результатов\n",
    "\n",
    "Для начала давайте взглянем на некоторые изображения, которые модель из L слоёв пометила неправильно. Это покажет несколько неправильно маркированных изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Несколько типов изображений, на которых модель обычно плохо работает:**  \n",
    "- Тело кошки в необычном положении\n",
    "- Кот появляется на фоне аналогичного цвета\n",
    "- Необычный окрас и вид кошек\n",
    "- Угол камеры\n",
    "- Яркость изображения\n",
    "- Изменение масштаба (на изображении кот очень большой или маленький)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Проверьте свое изображение ##\n",
    "\n",
    "Поздравляю с завершением этого задания. Вы можете использовать собственное изображение и увидеть результат работы вашей модели. Для этого:\n",
    " 1. Нажмите «Файл» в верхней панели этого блокнота, затем нажмите «Открыть».\n",
    " 2. Добавьте свое изображение в каталог этого блокнота Jupyter в папке «images».\n",
    " 3. Измените имя вашего изображения в следующем коде.\n",
    " 4. Запустите код и проверьте правильность алгоритма (1 = кот, 0 = не кот)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "## START CODE HERE ##\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = Image.open(fname).resize((num_px, num_px))\n",
    "my_image = np.asarray(image).flatten()\n",
    "my_image = my_image/255.\n",
    "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использованная литература**:  \n",
    "\n",
    "- для автоперезагрузки внешнего модуля: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
